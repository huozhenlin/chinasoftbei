---
date: 2017-06-20 20:46
status: draft
title: 数据捉取
---

# 代码包介绍
这里需要修改，把放置数据捉取py文件的列出来并声明其功能
# 数据来源
为了搜集所需的数据，我们对以下url进行了爬取
- 异常天气
    中国天气网<http://www.weather.com.cn/alarm/#>
    中央天气<http://m.nmc.cn/>
- 演唱会
    大麦网<https://s.damai.cn/ticket/sports.html>
    爱稻草<http://www.idaocao.com/>
- 新闻时政
    新浪网<http://news.sina.com.cn/china/>
    新华网<http://www.news.cn/politics/>
    
- 展会
   e展网<http://www.eshow365.com/> 
   中国展会网<http://www.china-show.net/>
- 体育赛事
  大麦网<https://s.damai.cn/ticket/sports.html>
  永乐票务<http://www.228.com.cn/category/tiyusaishi/>

# 主要使用到的python模块
| 模块 | 用途 |
| -------- | -------- |
| requests   | 进行网络请求   |
| BeautifulSoup   | 将网页源代码生成树状结构，便于抽取想要的内容   |
| selenium        |      自动化测试工具，用于请求网页中使用ajax技术调用的数据|
| re        |     通过正则表达式匹配想要获取的内容|

# 数据爬取流程
## 分析网络请求方式
通过观察url我们可以知道网页数据的请求方式，像下面这种为get请求

![](/中国软件杯/_image/2017-06-20-20-15-12.jpg)
对于get请求，我们将url拆分得到sid和page,sid为事件标记类型，在爱稻草中为演唱会事件，page为页数，因此，我们想请求前a页的数据，我们可以通过以下代码实现
```python
for i in range(1,int(a)+1):
        url="http://www.idaocao.com/yulenews/yanchu_sort.do?sid=8&page=%d"%i
        print url
        time.sleep(10)
        content=requests.get(url)
```
对于Post请求的数据，我们可以用到selenium工具辅助，像中国天气网中的异常天气数据是通过ajax动态请求的，我们使用到了selenium的api模拟点击请求数据

![](~/21-22-21.jpg)
> 部分代码如下
```python
def get_weather(self):
    #c=DB()
    driver=webdriver.PhantomJS()
    driver.get(self.url)
    import time
    time.sleep(20)
    driver.find_element_by_xpath("//*[@id='future6ForecastNav']/a[2]").click()
    time.sleep(10)
    frame=driver.find_element_by_xpath("//*[@id='tab_con1']/div[2]/iframe");
    driver.switch_to_frame(frame)
    # 获取网页源代码
    content = driver.page_source
    bs=BeautifulSoup(content,'html.parser')
    x=bs.find('div',class_='dDisasterAlarm').find_all('ul')
    ...
```
## 分析网页代码结构
Google浏览器提供了强大的开发者选项，按`F12`可调出，比如说在爱稻草网中我们要提取某个演唱会，如图，事件包含在`<li></li>`标签中，我们提取就要层层分析
![](/中国软件杯/_image/2017-06-20-20-24-20.jpg)
> 相关代码
```python
#开始捕获数据
def get_idaocao():
   (此处省略无关代码)
    bsobj=BeautifulSoup(content.content,'html.parser')
    try:
        li=bsobj.find('div',class_='mainList').find('ul',class_='wen').find_all('li')
        for i in li:
        href=i.find_all('a')[0]['href'][2:]
        href="http://www.idaocao.com"+href  #爱稻草网页中演唱会的链接
        title=i.find('span',class_='title').string#爱稻草网页中演唱会的标题
        data=i.find('span',class_='date').string#爱稻草网页中演唱会的时间
        num = a.select(table='sing', url=href)
    except Exception as e:
        print e.message
```
## 数据保存
我们爬取到数据后会把数据保存到Mysql数据中
![](/中国软件杯/_image/20170620213911.jpg)