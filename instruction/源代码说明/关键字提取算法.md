---
date: 2017-06-21 20:51
status: draft
title: 关键字提取，热度查询算法
---

# 关键字提取算法
## 算法说明
- 对应文件路径为：softbei/analyse_data/age/tf\_idf.py  

传统的自然语言处理技术中提取关键词的算法为经典的TF-IDF（Term Frequency–Inverse Document Frequency）算法。实际上是：TF * IDF，TF词频(Term Frequency)，IDF逆向文件频率(Inverse Document Frequency)。TF表示词条在文档d中出现的频率。IDF的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。

热门的tf-idf算法实现有结巴分词等等。这些tf-idf算法的实现依赖一个离线的idf词库，该词库中存储了大量词汇的idf值。但是，互联网新词汇层出不穷，这些新词汇往往不出现在离线的idf词库中。  

另外一些短语，诸如包含在《》，「」，（）等符号中的短语，不宜分词后再计算IDF。例如：《我的大学》，“我的大学”应该视为专有名词，若分词为“我 的 大学”再计算IDF则没有意义。另外演唱会等事件常常包含此类情况。若可以对这些专有名词直接计算IDF，则关键词提取可实现更高精度。但是，离线IDF库不可能包含所有专有名词。  

若一个词不出现在IDF词库中，其被挑选为关键词的概率会反常地变大，这就导致了关键词提取精度的下降。为了避免这个问题，我们运用大数据思维，利用百度搜索返回的结果数计算一个词的IDF。设要计算词汇w的IDF。首先利用百度搜索w，得到搜索结果的数量n(w)。例如搜索“杰伦”，搜索结果如下图，搜索结果的数量n(“杰伦”)=11900000。  

![](/算法说明/_image/Pic1.png)
  

定义大数据逆向文本率BDIDF(Big Data Inverse Document Frequency)如下式：  

![](/算法说明/_image/pic4.png)


其中n(w)为词汇对应的搜索结果的数量， （1亿）为百度搜索得到的最大搜索结果数量。例如n（“中国”）=，n（“上海”）=。我们把这个最大搜索结果数量作为文本总量使用。显然地BDIDF（“中国”），BDIDF（“杰伦”）。  

用以上BDIDF方法可以得出所有词汇的逆向文本率。本项目中关键字主要用于标题去重，而标题中某一词汇出现的频率一般为1，所以我们直接使用BDIDF取代TF-IDF，即一个词汇的重要程度等于1*BDIDF。选择1*BDIDF值最高的n个即为关键词。  

下面是本项目关键字提取的分词算法除了分词、去除停用词之外，还有提取专有名词的步骤，流程图如下： 

![](/算法说明/_image/pic2.png)
 

BDIDF算法实现较为简洁，流程图如下：  


![](/算法说明/_image/pic3.png)


## 代码实现

> 导入模块并分词代码实现

```python 
from pyltp import Segmentor
segmentor = Segmentor()
segmentor.load(r'C:\Users\hzl\PycharmProjects\untitled\chinasoftbei\analyse\ltp_data\cws.model')  # 分句模型
words = segmentor.segment(x)  #分词
```
> 将分好的词放到百度搜索引擎中获取搜素结果

```python
#返回关键词
def get_num(title):
    hot_words = []
    for x in title:
        # 去掉停用词
        if x not in stopwords:

            driver.get(url)
            driver.find_element_by_id('kw').send_keys(str(x).decode('utf8'))
            driver.find_element_by_id('su').click()
            time.sleep(3)
            bsobj = BeautifulSoup(driver.page_source, 'html.parser')
            try:
                div = bsobj.find('div', class_='nums').get_text().encode("utf-8")
                num = re.findall(r'约(.*)个$', div)[0]
                num = num.replace(',', '')
            except Exception as e:
                print 'not found'
                num = 100000000
            hot_num = math.log(100000000 / int(num))
            if hot_num > 1.0:
                hot_words.append(x)
    # 返回关键字
    return hot_words
  
'''
这个是对标题进行分词的模块
方法接收一个参数，就是待分词的csv地址
采用本地分词，效率高
顺便加强了分词效果，像一些专用名词，我们不会对其进行拆解
'''
def fenci(address):
    stopwords = {}.fromkeys([line.rstrip() for line in open('tingyongci')])  # 去掉停用词
    segmentor = Segmentor()
    segmentor.load(r'C:\Users\hzl\PycharmProjects\untitled\chinasoftbei\analyse\ltp_data\cws.model')  # 分句模型
    df = pd.read_csv(address)
    titles = list(df['标题'].head(30))
    f=open('tf_idf.txt','a')

    #将专用名词提取出来，像被'《》，“”，【】'等符号包括的
    for x in titles:
        special_word=[]
        try:
            kw= re.findall(r'“(.*)”',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw= re.findall(r'《(.*)》',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw= re.findall(r'【(.*)】',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw=re.findall(r'[(.*)]',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass


        words = segmentor.segment(x)  #分词
        for x in words:
            # 去掉停用词

            if x not in stopwords:
                print x
                get_num(str(x).decode('utf8'))
        for y in special_word:
                print y

        print '-------------->>>>>'
```


##细节优化
- 分词模块会把句子中所有字词进行拆分，这就导致一些专用名词也被拆分，要想这些专用名词不被拆分，有好多实现的方法，本文使用的方法是正则匹配，实现了了一些符号中的字词被保留，如“<font color="red""><>，“”，[]</font>”等等
> 部分代码实现

```python
        try:
            kw= re.findall(r'“(.*)”',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw= re.findall(r'《(.*)》',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw= re.findall(r'【(.*)】',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
        try:
            kw=re.findall(r'[(.*)]',x)[0]
            special_word.append(kw)
        except Exception as e:
            pass
```