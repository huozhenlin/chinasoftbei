---
date: 2017-06-20 20:53
status: draft
title: 重复事件去重算法实现
---

# 事件去重算法
## 算法说明
- 对应文件路径为：`softbei/csv_removal/class\_removal.py `
   
    从不同网站捉取回来的事件有些事相同的，去重后助于减轻我们今后对数据进行分析的量。本项目利用simhash进行去重。simhash是google用来处理海量文本去重的算法。Simhash能够将一个文档，最后转换成一个64位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是小于n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。  
本项目去重算法第一步：去除标题完全相同的事件。 第二步：计算各个事件的simhash值。 第三步：各获取两个样本之间的海明距离，根据经验，海明距离在0-3之间的算重复事件，判定为重复时间则去重。Simhash原理图如下：  

![](/算法说明/_image/2017-06-16-15-18-08.jpg)

本项目事件去重算法流程图如下：  

![](/算法说明/_image/pic9.png)



## 代码实现
```python
# coding:utf8
from simhash import Simhash
import sys

reload(sys)
sys.setdefaultencoding('utf-8')
import pandas as pd

'''
去重算法思路：
第一步：使用drop_duplicates()方法去掉dataframe中完全相同的标题
第二步：使用递归算法，层层循环遍历样本
第二步：使用simhash的distance()方法获取两个样本之间的海明距离，根据经验，海明距离在0-3之间的
        算重复事件
'''


# 封装去重算法，方法传入一个csv文件名，列名(单个),生成后的文件名
def drop_repet(csv_name, colunm, update_name):
    b = 0  # 起点

    # drop_duplicates()方法可用于快速去除相同的重复数据，python自带
    df = pd.read_csv(csv_name).drop_duplicates([colunm])

    # 将需要去重的列提取出来转换成list

    title = list(df[colunm])
    hash_title = []
    for ii in title:
        hash_title.append(Simhash(ii))

    # 两个list转成dict
    d = {}
    xx = []
    for i in range(len(title)):
        d[title[i]] = hash_title[i]

    # 递归算法,i为字典键
    for i in d:
        b = int(b) + 1
        for k in title[b:]:
            # 判断海明距离
            ss = d[i].distance(d[k])

            if 0 < ss <= 3:
                xx.append(k)

                print '找到重复事件:' + k

    title = []
    for x in set(xx):
        d.pop(x)
    for a in d:
        title.append(a)
    # 判断是否存在
    df = df[df[colunm].isin(title)]
    return df
    # 将去重后的数据保存到指定文件中
    # pd.DataFrame(df).to_csv(update_name,encoding='utf8',index=None)

```