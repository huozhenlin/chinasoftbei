#重复事件去重算法实现
- 背景:从不同网站捉取回来的事件有些事相同的，去重后有助于减轻我们今后对数据进行分析的量
- 参考链接:http://yanyiwu.com/work/2014/01/30/simhash-shi-xian-xiang-jie.html
##去重思路
- 第一步：使用drop_duplicates()方法去掉dataframe中完全相同的标题
- 第二步：使用递归算法，层层循环遍历样本
- 第三步：使用simhash的distance()方法获取两个样本之间的海明距离，根据经验，海明距离在0-3之间的算重复事件
- 图解:
   ![](./_image/2017-06-16-15-18-08.jpg)



>代码实现
```python
#封装去重算法，方法传入一个csv文件名，列名(单个),生成后的文件名
def drop_repet(csv_name,colunm,update_name):
    b=0 #起点

    #drop_duplicates()方法可用于快速去除相同的重复数据，python自带
    df = pd.read_csv(csv_name).drop_duplicates([colunm])

    #将需要去重的列提取出来转换成list

    title = list(df[colunm])
    hash_title=[]
    for ii in title:
        hash_title.append(Simhash(ii))

    #两个list转成dict
    d={}
    xx=[]
    for i in range(len(title)):
        d[title[i]]=hash_title[i]

    # 递归算法,i为字典键
    for w in title:
        b = int(b) + 1
        for k in title[b:]:
        # 判断海明距离
            ss = d[w].distance(d[k])
            if 0 < ss <= 3:
                xx.append(k)


    title=[]
    for x in xx:
        d.pop(x)
    for a in d:
        title.append(a)
    #判断是否存在
    df=df[df[colunm].isin(title)]
    #将去重后的数据保存到指定文件中
    pd.DataFrame(df).to_csv(update_name,encoding='utf8')
```